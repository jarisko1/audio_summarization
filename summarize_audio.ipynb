{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f5355d",
   "metadata": {},
   "source": [
    "# 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "AUDIO_EXTENSION = \".mp3\"\n",
    "STT_MODEL_ID = \"openai/whisper-large-v3-turbo\" # 7GB VRAM required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba6a19",
   "metadata": {},
   "source": [
    "## pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10738c",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ab4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU pip transformers datasets[audio] accelerate huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973edad",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7cc9d",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyter ipywidgets langchain azure-ai-inference python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d46b62",
   "metadata": {},
   "source": [
    "# 1. Extract text from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb53c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    STT_MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(STT_MODEL_ID)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "transcriptions = []\n",
    "\n",
    "for filename in tqdm(os.listdir(DATA_FOLDER), \"Extracting text from audio files\"):\n",
    "    if filename.endswith(AUDIO_EXTENSION):\n",
    "        file_path = os.path.join(DATA_FOLDER, filename)\n",
    "        result = pipe(file_path)\n",
    "        transcriptions.append({\n",
    "            \"filename\": filename,\n",
    "            \"text\": result[\"text\"] # type: ignore\n",
    "        })\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, \"transcriptions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transcriptions, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74127c",
   "metadata": {},
   "source": [
    "# 2. Summarize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b065add",
   "metadata": {},
   "source": [
    "## Load transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "AUDIO_EXTENSION = \".mp3\"\n",
    "STT_MODEL_ID = \"openai/whisper-large-v3-turbo\" # 7GB VRAM required\n",
    "SUMMARIZATION_MODEL_ID = \"facebook/bart-large-cnn\"\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, \"transcriptions.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(transcriptions)} transcriptions:\")\n",
    "    for transcription in transcriptions:\n",
    "        print(f\"\\t{transcription['filename']} with lenght {len(encoding.encode(transcription['text']))} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4e6d0",
   "metadata": {},
   "source": [
    "## Option 1: Foundation model summarization\n",
    "> GPT-4.1 via GutHub models (limit 8k input context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model = \"openai/gpt-4.1\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=30000,\n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "for transcription in transcriptions:\n",
    "\n",
    "    transcription[\"summary\"] = \"\"\n",
    "\n",
    "    chunks = splitter.split_text(transcription[\"text\"])\n",
    "\n",
    "    for chunk in chunks:\n",
    "\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(\"Your task is to summarize the text provided in the next message. \"\n",
    "                            \"Focus on practical information, key points, and actionable insights. \"\n",
    "                            \"Ignore any personal opinions, anecdotes, or irrelevant details. \"\n",
    "                            \"Look also for information about sunglasses that allow some UV light to pass through. \"),\n",
    "                UserMessage(chunk),\n",
    "            ],\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "        transcription[\"summary\"] += response.choices[0].message.content + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(transcriptions[0][\"summary\"], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43e524",
   "metadata": {},
   "source": [
    "## Option 2: Local summarization model\n",
    "> BART\n",
    "- No option to specify how the summary should look like\n",
    "- Very limited context window (2K or so)\n",
    "- These 2 combined produce very long summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "model = \"facebook/bart-large-cnn\" # 3 GB VRAM required for batch size 1\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model)\n",
    "\n",
    "for transcription in tqdm(transcriptions, \"Summarizing transcriptions\"):\n",
    "\n",
    "    text = transcription[\"text\"]\n",
    "\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    transcription[\"summaries\"] = summarizer(\n",
    "        chunks,\n",
    "        batch_size=16,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    transcription[\"summary\"] = \" \".join(\n",
    "        [summary[\"summary_text\"] for summary in transcription[\"summaries\"]]\n",
    "    )\n",
    "    print(f\"Summary lenght: {len(transcription['summary'])} from original {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(str(transcriptions[0][\"summary\"]), width=80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
